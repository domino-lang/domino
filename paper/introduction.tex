\section{Introduction}
%The paradigm of \emph{reduction-based security}, a.k.a. \emph{provable security}, is the state-of-the-art method of understanding and confirming the security of cryptographic constructions based on well-studied assumptions. Concretely, 
A reduction proof reduces the security of a cryptographic construction (in a model) to the security of the underlying building blocks (in a model). Writing reductions, arguing about their soundness and verifying one's own (or someone else's) proof is relatively easy in some contexts, but it can get out of hand when constructions and security arguments become complex. Thus, the community develops methods to tame the complexity of security proofs and to increase their precision, including, e.g., \emph{game-hopping}~\cite{EPRINT:Shoup04}, \emph{code-based} arguments~\cite{EC:BelRog06} as well as composition frameworks such as Universal Composability (UC~\cite{FOCS:Canetti01}), Constructive Cryptography (CC~\cite{FC:Maurer10}) and state-separating proofs (SSPs~\cite{X}) and a general culture of using black-box security notions as intermediate abstractions.

A further, complementary approach supports mathematicians in their proof verification endeavour
using computer assistance. Formal computer-verification of one's sequences of game-hops and reduction proofs certainly provides additional guarantees. However, Halevi~\cite{X}, in his position paper, was hoping for significantly more: Proof-assisting algorithms, so he hoped, could \emph{reduce} the overall amount of required work, especially \emph{verifying} the soundness of reductions (which he viewed as tedious) while \emph{designing} reductions (which he viewed as creative) remains in the hands of humans.

%Nevertheless, it is sometimes desirable to formally verify one's sequences of game-hops and reduction proofs. One the other hand, this verification should not require additional work on top of the written pen-and-paper proof, but rather, it would be desirable to \emph{reduce} the amount of overall required work and discharge some amount of work to a computer. At least, this is how Halevi imagined this process in his position paper~\cite{X}, and indeed, his vision is very appealing. 

Inspired by Halevi~\cite{X}, Shoup~\cite{EPRINT:Shoup04}, and Bellare-Rogaway~\cite{EC:BelRog06},
CertiCrypt~\cite{X} and its follow-up work EasyCrypt~\cite{X} pioneered the development of formal
verification tools specifically aimed at cryptographic reduction proofs, so cryptographers would
 be able to write and analyze pseudo-code directly in a tailor-made tool for cryptography rather 
than encode their constructions and reductions into generic formal verification methods. 
Unfortunately, EasyCrypt proofs nevertheless
require a significant additional amount of work (see Section~\ref{X} for details) compared to a pen-and-paper-only proof. Thus, while originally, it seemed plausible that EasyCrypt would be rather widely adopted by cryptographers~\cite[page 1]{X}, the additional required work still constitute a significant barrier.

Thus, we set out to reduce this barrier. However, rather than aiming for the same expressiveness and power as EasyCrypt, we target only a subset of cryptographic reductions, known as \emph{straightline reductions} for distinguishing games, where the reduction only runs the adversary once and returns whichever bit the adversary returns. Moreover, we require that the reduction \emph{perfectly} emulates the game(s) which the adversary expects. Such proofs are, e.g., very common in key exchange analysis, see, e.g.~\cite{X} for an example\footnote{The only exception in the cited papers are the proof steps where the game aborts upon nonce collisions. In our framework, such an abort step can be integrated as an assumption (rather than be proven in the tool).}. In this more restricted setting, we show that,
indeed, only a much smaller amount of addition manual work is required compared to EasyCrypt. Section~\ref{sec:easycrypt} reviews EasyCrypt, Section~\ref{sec:ssbee} presents our new formal verification tool SSBee and Section~\ref{sec:casestudies} discusses case studies.

\subsection{EasyCrpyt}\label{sec:easycrypt}
EasyCrypt has been modeled to capture code-based game-hopping proofs such as Bellare and Rogaway's proof for ..~\cite{EC:BelRog06}. The latter requires careful comparison of distributions in order to bound the statistical distances between them. As a result, EasyCrypt incorporates a very precise and powerful logic which allows to argue about statistical differences. For the simple case of perfect straightline reductions, the workflow in EasyCrypt is as follows. Assume that we have a reduction $\rdv$ which transforms any adversary $\adv$ who tries to distinguish between game $\M{Game}_\text{big}^0$ and $\M{Game}_\text{big}^1$ into an adversary $\bdv:=\adv\rightarrow\rdv$ who tries to distinguish between 
$\M{Game}_\text{small}^0$ and $\M{Game}_\text{small}^1$. Now, to verify that the reduction is sound, we  need to show that $\rdv$ composed with $\M{Game}_\text{small}^b$ behaves as $\M{Game}_\text{big}^b$ for both $b\in\bin$. Let us denote $\rdv$ composed with $\M{Game}_\text{small}^b$ as $\rdv\rightarrow\M{Game}_\text{small}^b$.

The EasyCrypt proof proceeds by induction. We define a state relation $\mathsf{Rel}$ between the state of 
$\rdv\rightarrow\M{Game}_\text{small}^b$ and the state of $\M{Game}_\text{big}^b$. As induction start, we show that $\mathsf{Rel}$ holds on the starting states of both games. As induction step, we show that if $\mathsf{Rel}$ holds on the states of $\rdv\rightarrow\M{Game}_\text{small}^b$ and $\M{Game}_\text{big}^b$ and if we make the same query to both games, then $\mathsf{Rel}$ holds on the new resulting states of
$\rdv\rightarrow\M{Game}_\text{small}^b$ and $\M{Game}_\text{big}^b$ after the query \emph{and} both games produce the same output. By this induction argument, we can conclude that for any sequence of queries, both $\rdv\rightarrow\M{Game}_\text{small}^b$ and $\M{Game}_\text{big}^b$ always returns the same answers.

Hence, the workflow for verifying the reduction consists of
\begin{description}
\item[(1)] defining the state relation $\mathsf{Rel}$.
\item[(2)] proving the induction start.
\item[(3)] proving the induction step.
\end{description}
Thus, verifying a reduction in EasyCrypt requires a significant additional amount of detail
compared to a pen-and-paper-only proof.

%For (2) and (3), EasyCrypt provides its own proof language.

\subsection{SSBee}\label{sec:ssbee}
We now turn to our contribution: The development of the tool SSBee. As hinted before, SSBee
has similar goals as EasyCrypt, but restricted to the setting of perfect straightline
reductions. Compared to EasyCrypt, the user still needs to define the state relation $\mathsf{Rel}$, but does not need to prove (2) and (3) anymore, since they are discharged to an SMT-solver.
Since state relations are typically relatively easy to determine (because $\rdv\rightarrow\M{Game}_\text{small}^b$ typically have the ``same'' state as $\M{Game}_\text{big}^b$, just distributed across the reduction $\rdv$ and the game $\M{Game}_\text{small}^b$), we thus postulate that writing formally verified reduction proofs in SSBee typically constitutes less or a comparable amount of work compared to carrying out the same proof on pen-and-paper-only---once the user learnt how to use SSBee.

Concretely, as for EasyCrypt, the user first writes the games $\M{Game}_\text{big}^b$ and $\M{Game}_\text{small}^b$ as well as the reduction $\rdv$ in SSBee pseudo-code. Next the user describes the state relation $\mathsf{Rel}$ in SMTLib code. SMTLib~\cite{X} is a joint standard interface supported by multiple SMT solvers. SSBee currently relies either on Z3~\cite{X} or CVC4~\cite{X}, but SMTLib enables adopting other SMT solvers later as well. For the case that $\M{Game}_\text{big}^b=\M{Game}_\text{small}^b$ and the reduction $\rdv$ is trivial (i.e., just forwards messages), the state relation in SMTLib code would be written as:
\[\texttt{(= state-game-big-b state-game-small-b)},\]
SMTLib is a Lisp dialect~\cite{X} which first states the operator ``='' before both of its arguments.
While Lisp is very different from the imperative style typically used in cryptographic articles,
we opted for asking the user to directly specify invariants in SMTLib, because implementing a compiler
which translates invariants written in pseudo-code to SMTLib would create a barrier between the user and the SMT solver and because writing directly in SMTLib allows a proficient user to rely on the full expressivity of SMTLib if the user wishes to.

\paragraph{Randomness Mapping}
In addition to providing $\mathsf{Rel}$, both EasyCrypt and SSBee proofs additionally require to
map \emph{randomness} between two games... [TO DO]

\paragraph{State-Separating Proofs}
SSBee is able to verify arbitrary (perfect) straightline reductions between distinguishing games as outlined above, and, additionally, provides support for the use of \emph{state-separating proofs (SSPs)}~\cite{X}. The latter are convenient in the context of formal verification since they reduce the
number of code-equivalence proofs (and thus the number of relations the user needs to define). Concretely, SSPs (see Section~\ref{X} for more details and Kohbrok~\cite{X} for a comprehensive introduction) allow to split a game into multiple stateful
pieces of code---\emph{packages}---which call one another via \emph{oracles}. In SSP-style proofs,
the adversary $\adv$ is always viewed as the ``orchestrator'' (on the left) which can activate other
packages via oracle calls which, in turn, can activate other packages via oracle calls. In SSPs,
each callee eventually returns an answer to its caller, call graphs are acyclic and self-calls are forbidden, leading to a simpler activation pattern as, e.g., in UC (cf. Unruh~\cite{X}). The game ends when the adversary returns a bit $b^*$ and advantages are defined as 
\begin{align*}
\mathsf{Adv}(\adv,...)&:=...\\
\mathsf{Adv}(\bdv,...)&:=...
\end{align*}
TODO: Something about graphs, graph exports, code exports.



\subsection{Case studies}\label{sec:casestudies}
In order to illustrate the use of SSBee, we now present two
case studies: A reduction-based analysis of Yao's garbled Circuits approach
as well as a reduction-based analysis of combining a pseudorandom function
with authenticated encryption in order to build a key exchange protocol with
key confirmation. We chose examples from the areas of key exchange and the
areas of secure multi-party computation (MPC) since both domains tend to
handle complex large protocols which makes tooling for proof development
and verification more appealing to them so that it seems useful to exemplify
how to use SSBee in these two domains.

\paragraph{Yao's Garbled Circuits}
Yao's garbled circuit approach is one of the first general MPC techniques.
Here, a garbler garbles a circuit $C$ into $\tilde{C}$ as well as the
evaluator's input $x$ into $\tilde{x}$. The evaluator can obtain
$y=C(x)$ by evaluating the garbled circuit $\tilde{C}$ on the garbled
input $\tilde{x}$. Security requires that $\tilde{x}$ and $\tilde{C}$
can be simulated given only the output $y$ (as well as the size and
topology of $C$).

Yao's Garbled Circuits have first been formally analyzed by Lindell and Pinkas~\cite{X}.
Later, garbling schemes have been abstracted out as its own primitive by 
Bellare, Hoang and Rogaway~\cite{X} who provided a more modular analysis of
Yao's garbling scheme using pseudo-code. Moreover, Yao's Garbled Circuit approach has also
been studied in EasyCrypt by ...~\cite{X} and by Brzuska and Oechsner~\cite{X}
using SSPs. We will use the 4 previous works as a base of comparison with our SSBee proof.

Concretely, in this introduction, we would like to focus on one concrete
proof step which is common to all proofs, including ours. Namely, all proofs
proceed via polynomially many game-hops which change how the garbling $(\tilde{x},\tilde{C})$ 
is computed---however, at this point, the input value $x$ is still used to compute
$(\tilde{x},\tilde{C})$. A core argument, then, is that the \emph{identical}
distribution $(\tilde{x},\tilde{C})$ can be generated using only $y$. Lindell
and Pinkas coined this game-hop a \emph{semantic switch} {\color{blue}CHECK WHO SAID THIS}
and provide an explanation consisting of 10 lines of text (\cite[page 23]{X}). The
analogous conceptual argument by Bellare, Hoang and Rogaway consists of 4.5 lines of text (\cite[bottom of page 22]{X}). Both provide a conceptual high-level argument about distributions of keys. In turn,  the EasyCrypt analysis additionally makes explicit which randomness for key sampling in one game corresponds to randomness for key sampling in the other game and, based on this, proves that
if both games receive the same input, they produce the same output---note that garbling games are
stateless, so no state relation needs to be provided in this case {\color{blue}True that they use
something stateless?}. The EasyCrypt proof consists of ... lines of code (~\cite[page x]{X}).
Finally, in Brzuska's and Oechsner's proof, this code equivalence proof consists of six long
columns of pseudo-code with additional discussion of ... (~\cite[page x-y]{X}). Note that
Brzuska and Oechsner conduct the proof on a \emph{stateful} game since they consider a sub-game.
Our SSBee proof formalizes the SSP-based Brzuska-Oechsner approach and thus, this proof step
 also requires the definition of both a randomness relation and a state relation. However,
no further argument is required. While we need one additional relation compared to the EasyCrypt
proof, our SSBee-based proof does not require proving code-equivalence based on the relation
(since this succeeds automatically using an SMT-solver). Moreover, our SSBee proof compares
very favorably with the long analysis of Brzuska and Oechsner (and is additionally more precise)
and arguably does not require more work than the pen-and-paper proofs by Lindell-Pinkas
and Bellare, Hoang and Rogaway. Since lines of code and text are not necessarily useful measures
of complexity, we can also consider the conceptual arguments needed to be carried out in all the
proofs. Interestingly, the \emph{randomness mapping} in SSBee and EasyCrypt and the argument
about key distributions in Pinkas-Lindell and Bellare-Hoang-Rogaway are essentially the same
argument. In turn, in Brzuska-Oechsner, the proof proceeds more syntactically, implicitly mapping
different samplings onto each other.

Since we view SSBee as a tool not only to verify proofs but also to support \emph{proof development},
we also extend the work by Brzuska and Oechsner to compose Yao's garbling scheme with a random oblivious transfer protocol (OT) into a secure 2-party computation protocol with semi-honest security. This additional composition step has already been proven by Lindell and Pinkas~\cite{X} and formalized in EasyCrypt~\cite{X}, but not yet been formalized in state-separating proofs. Interestingly, it turns out that the modular garbling games used by Brzuska and Oechsner compose more conveniently (using the
SSP approach) than the standard formulation of (selective) garbling scheme security. See Section~\ref{X} for details. To carry out the extension of the garbling proof, we first sketched package graphs in an online tool called ... to explore the composition with OT. To support this workflow, it seems a useful
extension for future work to export package graphs not only to tikz for research articles, but also
to ... for manually editing graph sketches.






\iffalse
=== stopped here, please do not continue reading ===

Nevertheless, our experience is that analyzing a large cryptographic protocol remains difficult.
%In particular, UC and CC as well as the culture of abstract security notions in cryptography help reduce the \emph{conceptual} complexity of a concrete protocol. Namely, we can replace a concrete, complex protocol building block by a more simple ideal functionality or a black-box building block with well-defined security properties. %such as symmetric encryption under indistinguishability under chosen plaintext attacks (IND-CPA) rather than, say, AES is 
%Conveniently, this enables us to build large protocols based on black-box components which are, by themselves, complex (cf. constructions of key encapsulation mechanisms secure against chosen-ciphertext attacks~\cite{X} or, for a more extreme example, indistinguishabilility obfuscation~\cite{X}), but have an easy-to-use interface.
%Unfortunately, sometimes
%===
%Nevertheless, studying protocols such as Transport Layer Security (TLS~\cite{TLS13})
%and Messaging Layer Security (MLS~\cite{MLS}) \emph{as they are} 
%%(without simplifications and abstractions) 
%remains difficult, since their description exceeds 100 pages which makes the protocols unwieldy for analysis without simplifications. Similarly, other real-life systems tend to combine multiple cryptographic components in complex dependencies,
%see, e.g., remote attestation mechanisms~\cite{https://arxiv.org/pdf/2102.08804.pdf,https://en.wikipedia.org/wiki/Trusted_Computing}, ... or ... .
In particular, we encountered the following challenges when conducting pen-and-paper analyses of
large protocols.
\begin{description}
\item[(1) Large protocols and reductions.] A protocol description can be outrageously long (> 100 pages), even when abstracting concrete building blocks into black-box notions (cf. TLS~\cite{TLS13} and MLS~\cite{MLS}). Reductions typically emulate most of a protocol and thus suffer from the same issue.
\item[(2) Consistent code-base.] Protocol and game components are referred to multiple times in an analysis and maintaining a consistent code-based is difficult. Concretely, intermediate 
security notions (e.g., hybrid games) and protocol descriptions (e.g., those used in hybrid games) evolve
during proof development.
\item[(3) Verifying reductions.] Verifying that reductions emulate the protocol correctly is challenging due to the size of the reduction and the protocol, even when the reduction is conceptually simple. This difficulty holds both for a writer and a reader of the reduction.
\end{description}
None of the above (except for maybe (3)) constitutes a traditional, conceptual challenge in the field of cryptography. In particular, a large protocol description can be viewed as a symptom of unnecessarily complicated design, and it is typically preferable to simplify the design rather than put up with a complicated protocol and analysis. The reality of protocols such as TLS~\cite{TLS13} and MLS~\cite{MLS} is, however, that they serve many different goals simultaneously and their intermediate drafts as well as the final protocols themselves emerge from social compromise. 
Thus, if we consider it important to understand and analyze incredibly large (over-)complicated real-
life protocols comprehensively with reduction-based analysis, we need a method to do so.

\paragraph{The need for a new formal verification tool.}

Naturally, both, protocol code and reduction code can treated 
as code associated to a paper ra



Item (1) and (2) can be mitigated by describing the protocol
modularly and maintaining a single code-base for the protocol
whose components are imported into the analysis whenever needed.
However, typically, an important component of protocol analysis 
is to re-write a protocol (and the game around it) in a 
functionally equivalent way that is easier
to analyze (cf. Bellare-Rogaway code-based
game playing~\cite{EC:BelRog06}).
% or matches the shape of the reduction---in a way, 
Similarly, writing a reduction typically includes re-writing the protocol 
except for a small component which, instead of being locally evaluated, 
is forwarded to an oracle by the reduction. Establishing (perfect) soundness of 
a reduction is then, again, a proof of functional equivalence\footnote{Large 
real-life protocols typically only employ straightline reductions. The 
above proof outline does not apply to, e.g., rewinding, but it would 
allow, say, programming a random oracle. Also note that proving functional equivalence 
up to statistical difference can be avoided by first making a game-hop
bounded by a statistical difference.}. Thus, protocol analysis regularly
requires us to prove functional equivalence between different protocol descriptions,
reductions and games.
% Thus, we need to prove functional equivalence between 
%large protocol components. Similarly, for reductions, we need to
%prove that a reduction emulates a high-level game correctly while
%interacting with a low-level game that it tries to win. These proofs, too,
%are functional equivalence proofs for complex objects.

\paragraph{Verifying Code-Equivalence.}
Indeed, the core component of the pioneering formal
verification tools CertiCrypt~\cite{X} and EasyCrypt~\cite{X} for 
cryptographic reductions are functional equivalence proofs. Since functional 
equivalence is coNP-complete\footnote{An input $x$ such that 
$f(x)\neq g(x)$ shows that $f$ and $g$ are not code-equivalent.} and 
for randomized programs even higher up the polynomial hierarchy\footnote{We need to prove that the \emph{distributions} $f(x)$ and $g(x)$ are identical.}, 
CertiCrypt and EasyCrypt do not verify functional equivalences automatically.
Instead, functional equivalence is proven interactively: The user writes
a proof consisting of small, detailed steps which the tool verifies.
%but rather require the user to write a proof of functional equivalence
%consisting of small, detailed steps. 
As an example, the EasyCrypt formalization
of a proof for Yao's garbled circuits~\cite{X} shows that after polynomially many game-hops, a simulated garbling can be computed without knowing the input and while only knowing the output of the circuit evaluation, thus establishing the desired goal that a garbled circuit does not leak any information beyond its output value (which can
always be trivially derived, see~\cite{X} for more discussion). This code equivalence proof between two versions of the garbling, one with and one without
knowing the input, consists of ... lines of code in EasyCrypt (~\cite[page x]{X}).
%EasyCrypt,
% thus, indeed enables us to verify complex code equivalence proofs.

In turn, in the pen-and-paper proof of Yao's garbled circuits by Lindell and Pinkas, the analogous conceptual argument consists of 10 lines of text (\cite[page 23]{X}); and in the pen-and-paper proof of Yao's garbling scheme by Bellare, Hoang and Rogaway, it consists of 4.5 lines of text (\cite[bottom of page 22]{X}). Both code equivalence proofs consists of a conceptual high-level argument about distributions of keys, while the EasyCrypt analysis (a) additionally makes explicit which randomness for key sampling in one game corresponds to randomness for key sampling in the other game and (b) ... .

\paragraph{Proving Code-Equivalence Proofs.}
The aforementioned example of Yao's garbling scheme illustrates that even
exemplary detailed pen-and-paper proofs are much shorter than computer-verified
proofs of the analogous statement. Thus, while computer-verified proofs provide
higher assurance, they also require significant additional detail and, in their
current state, cannot be said to reduce workload, but rather increase it.

%current
%computer-verified proofs provide higher assurance, but do not reduce the
%workload in analyzing a complex protocol, but instead
%require significant additional work beyond the amount of detail typically 
%worked out in pen-and-paper proofs (Note that the above pen-and-paper

Thus, a natural question is whether computer-assistance cannot only help to
\emph{verify proofs} of code-equivalence, but perhaps also 
\emph{generate proofs} of code-equivalence automatically. At first sight,
this might seem overly optimistic, since program equivalence is an coNP-hard problem.
However, SMT-solvers such as Z3~\cite{X} and CVC4~\cite{X} excel in 
solving structured, practical instances of coNP-complete problems, and
the code-equivalences which appear in pen-and-paper proofs are of the type that 
they are treated as ``obvious'' to the human observer. Therefore, it seems
worth investigating whether this kind of ``obvious-to-humans'' code equivalence
is feasible for an SMT-solver as well and, perhaps, whether SMT-solver can also prove code equivalences that are not entirely obvious to the human observer.


% so it
%seems worth investigating whether program equivalence as it emerges in
%the context of cryptographic reductions could be solvable by an off-the-shelf
%SMT-solver. %Jumping ahead, in this article, we give evidence that proving
%code-equivalences can be significantly automatized, see Section~\ref{X}.

\paragraph{Managing code and incremental changes.}
Returning to item (1)-(3) above, proofs for large protocols require, in fact,
a large amount of code which needs to be maintained consistently: In terms of
basic objects, there are the protocol, the game describing its security as well
 as the games describing the underlying assumptions. Furthermore, a game-hopping
proof typically requires describing hybrid games, each of them with an inlined
version of the protocol where parts of the component might have been replaced
by idealized versions, e.g., a truly random function instead of a pseudorandom function (PRF). Additionally, each game hop comes with the description of a reduction.

The result of (successful) proof development is an article where all of the
above are consistent and where equivalent descriptions of a game have
been proven equivalent and reductions have been proven to be sound.
However, the \emph{development phase} proceeds via many stages where
we prove statements about games which later turn out not useful or
which have small errors and inconsistencies that we correct later.

When considering code modifications to correct errors, these changes do not
propagate automatically---in fact, we need to backtrack which pieces
of code are affected, which code-equivalence statements fail now etc..
Under the assumption that off-the-shelf SMT solvers can verify
code-equivalence statements and under the assumption that we have
made all reductions explicit in some formal language, we can, at 
any point of time, ask an SMT solver to (re-)prove all statements and thus see which
statements are affected by a modification. However, since most proofs
of large real-life protocols consists of many game-hops, we usually only
specify the hybrid games in pseudocode, but not the reductions, and
writing the reductions seems undesirable overhead, even if in return, we
get the (potential) benefit of automated verification and continuous 
integration.

\paragraph{State-separating proofs.}
An exception to the practice of incomplete game and reduction descriptions for
real-life protocol analysis is the state-separating proof methodology (SSP),
introduced by Brzuska, Delignat-Lavaud, Fournet, Kohbrok and Kohlweiss (BDFKK~\cite{X}). SSPs suggest to write code of games and protocols modularly in (stateful) packages which call one another via oracles but cannot access each others state other than
via oracle calls (whence their name). 

Modular code writing and code re-use has already been suggested in Bellare and Rogaway's original work~\cite{X} (TODO: check that this is true.) and been practiced, e.g., by BHR~\cite{X}, where TODO: example.
The SSP methodology (cf.~\cite{X,Y,Z}) develops this idea systematically further and ties it in closely with \emph{compositional} aspects. Concretely, an SSP package typically consists of code and state that is \emph{shared} between multiple cryptographic primitives and/or their games. Subsequently, we can specify reductions precisely in pseudo-code \emph{without} writing any new code, simply by re-using existing packages and composing them into a call-graph. Indeed, all existing works on SSPs make use of this feature~\cite{blankedcite}.

A downside to defining games via call-graphs of packages is a fragmented
description of games: To fully understand a game, we need to consider the
call-graph at the same time as the code of its building blocks, leading
to challenges in the presentation (cf.~\cite{ACNS:BEP24} for a discussion).
Moreover, the code of a single package is now re-used by many different games, 
so that changing the code of a package affects many definitions and proof 
steps at the same time.

Thus, SSPs would benefit from automated sanity checks such as oracle signatures
(the caller of an oracle includes the arguments that the callee expects) as
well as the aforementioned continuous integration approach.
Additionally, SSPs come with a high level of precision and
that is suitable for formalization. Hence, all-in-all, SSPs are an ideal
candidate to provide a tool-based development process
that is \emph{less} work than SSP pen-and-paper proofs %counterpart
and at the same time leads to computer-verified proofs.

%, automatization and,
%all in all, for using tools that not only \emph{verify}
%proofs, but actually support a tool-based proof development process
%that \emph{less} work than its pen-and-paper counterpart.

\subsection{Our contribution.} After a major development effort,
we present SSBee, a novel tool to support SSP-style proofs. 
In SSBee, the user can specify packages and call graphs in a 
pseudocode-style (see Section~\ref{X} for details), while SSBee
checks the consistency of the resulting composition. Subsequently,
the user can specify sequences of games, where each game-hop consists
either of a code-equivalence step or a reduction to an underlying
assumption. Since we consider SSPs, the latter merely consists in
specifying a sub-graph (and verification is not a major challenge).
As for EasyCrypt, the main challenge in SSBee are code equivalence proofs.
We elaborate on code equivalence proofs shortly, but first would
like to emphasize that the use of SSPs \emph{reduces} the number
of code-equivalence proofs, since a suitable modularization of a game
typically only requires one code-equivalence proof in the beginning
and one code-equivalence proof in the end of a proof sequence.
See Section~\ref{X} and Kohbrok~\cite{Kohbrok:thesis} for a details.

\paragraph{Code-Equivalence Proofs}






present SSBee, a tool that does all the things
we said above would be great. It maintains code for SSPs, checks code-equivalences
semi-automatically and provides continuous integration support.

On a technical level, we support all sorts of things in code, including for loops
and poly many hybrids.

On a user interface level, we can export to LaTeX and proof viewer, both graphs and packages which might make it easy to communicate what one proved in a paper but
deferring the (consistent) things to the prover.


%to support the development
%of proofs in SSP-style.


% as well as (partial) automatization of code-equivalence
%proofs via SMT-solvers.




%other works, where to go from here
%---in fact, they have been formalized both in EasyCrypt~\cite{X}
%and Coq~\cite{X}.

%The code-reuse allows to define new games and reductions precisely, and
%verifying cuts in graphs 






TODO:
- Mention (again?) that SSPs reduce code-equivalences.
- Mention that EasyCrypt does stat-gaps as well. 
- using SMT-solver for hybrid indices, um zu schauen dass sie bijektiv




\iffalse
=== OLD ===

Designing cryptographic constructions, reductions, proving their soundness and verifying the correctness of one's proofs is, at times, a difficult endeavour, because constructions can be complex (e.g., TLS 1.3~\cite{X}), reductions can be complex (even for simple constructions such as the Goldreich-Levin hardcore bit~\cite{X}), proving the soundness of a reduction can be intricate and subtle even for simple reductions (e.g., the random embedding reduction in Yao's weak-to-strong one-way function amplification), and verifying the overall proof is typically difficult because we tend to use significant amount of conceptual high-level reasoning which one needs to reproduce when verifying the proof (e.g., understanding why/how the orginal Lindell-Pinkas reduction proof for Yao's garbling scheme~\cite{X} uses an additionally provided encryption oracle of the underlying security notion for symmetric encryption and why this suffices to conduct a hybrid argument over the circuit in topological order).

\subsection{Supporting protocol and proof development and verification}
We categorize research on supporting these different stages of designing a protocol and developing and verifying the associated reduction proofs into the following four (overlapping) categories (1) attack-finding, (2) composition, (3) proof-writing styles and (4) formal verification and now briefly review each.

{\color{blue}
\paragraph{Attack-finding.} Tamarin (+ application to TLS, MLS), and why do I call this attack-finding (one can also call this automated proving, but should rememember that the adversary can only do a certain kind of operation). Briefly mention that the gap between the symbolic approach and the reduction-based approach can, in principle be bridged (computational soundness, Abadi-Rogaway), and that there is some research in this direction of doing this (still today).

\paragraph{Composition.} UC, random systems, constructive crypto, SSPs}

\paragraph{Proof-writing styles.}
%Cryptographic proof-writing style has significantly evolved throughout the
%development of the field. In 2004, 
Shoup explicates \emph{game-hopping} in order to advertise its use as a tool for taming complexity in security proofs. 
%In 2005,
Bellare and Rogaway introduce and advertise a \emph{code-based} variant of
game-hopping, treating code as semi-formal objects. 
%20 years later, 
%most reduction proofs in the field are structured into sequences of games, and a
%significant portion of them is also \emph{code-based}. 
The aforementioned state-separating proofs can be viewed as a refinement of code-based game-hopping which allow to construct reductions as cuts in graphs, so that proving/verifying the soundness of a reduction boils down to observing equality call graphs consisting of code-packages.

\paragraph{Formal verification of reductions.} 
%While attack-finding tools such as Tamarin also establish the absence of attacks in a symbolic model with idealized cryptographic primitives, Tamarin cannot help with verifying reduction proofs, as mentioned above. 
Interactive theorem provers such as Coq~\cite{X} allow one to computer-verify a
proof as long as it is written as a sufficient level of precision of detail.
The pseudo-code-based approach by Bellare and Rogaway explicates a proof style which is suitable for actual formalization, since all games and reductions are explicitly written in pseudo-code. Inspired by Bellare and Rogaway, in a pioneering work, X, Y and Z developed CertiCrypt~\cite{X}, the first approach for verifying cryptographic reductions, followed by A, B and C developing EasyCypt~\cite{X} as a first tool \emph{independent} of Coq. One of the important technical contributions of CertiCrypt/EasyCrypt is a method for proving the \emph{soundness} of a reduction which requires proving that two stateful, randomized, interactive programs have identical input-output behaviour. While research articles usually use high-level conceptual reasoning for this purpose, computers need more detailed and precise arguments, and a priori, it is unclear how to translate the high-level reasoning into a sufficiently formal and detailed language. We provide more technical details on how to successfully resolve this issue in Section~\ref{X}.

\subsection{Halevi's vision and beyond}
Halevi~\cite{X} joins Shoup, Bellare and Rogaway in the quest for
high-quality proofs by presenting a dream-vision of computer-assisted 
cryptographic proving. Namely, Halevi perceives designing cryptographic 
schemes and reductions as the \emph{creative} part of cryptographic research, 
while he perceives checking the soundness of reductions as \emph{mundane}. 
Thus, Halevi hopes for assistance in the development of proofs, relieving 
the researchers of work perceived as tedious: Checking the soundness 
of reductions shall be carried out by computers \emph{automatically}.

Unfortunately, computer-verification requires a detailed presentation of a 
proof argument in order to verify it and thus, while the current state-of-the-art
indeed allows to check the soundness of a reduction, it is far from automatic.
Instead, the user needs to provided a detailed proof of the soundness of the
reduction which is then verified---and thus, currently, using computer-verification
to check reduction proofs is very far from reducing work, let alone relieving
cryptographers of the work required for checking the soundness of a reduction.

A very useful question, closely related to Halevi's vision, is how
computer-assistance can generally relieve cryptographers from work perceived 
as tedious or difficult. %For example, Tamarin~\cite{X} checks the high-level
%logic of complex protocols.
As a simple example, maintaining pseudocode for games and reductions in a single 
code-base prevents inconsistency across a research article where the same code
is used in multiple places. Similarly, certain sanity checks on a reduction
are quite easy to carry out such as whether the reduction correctly accesses the
oracles of the underlying assumption. Moreover, while in general, proving the soundness of a reduction requires hard-to-formalize conceptual reasoning, the aforementioned graph-based SSP reductions merely require checking graph equality,
which is tedious for a human when the graphs are large, but easy for a computer in the case of the layered directed acyclic graphs (DAGs) used in SSPs. {\color{blue} Not sure that this is true, but it sounds nice. Somewhere, there should be an SSP intro, not sure yet where. In the text below, please assume that SSPs have been properly introduced---but I still need to find a place for them, maybe discuss proof viewer.}
\fi

\subsection{Our Contribution}
We develop \emph{SSBee}, an independent tool which allows to specify and maintain
SSP packages in pseudocode, compose them into games as well as write and verify
SSP-based game-hopping proofs. Additionally, SSBee visulizes all games and proofs in the SSP proof viewer~\cite{X} and exports pseudocode to LaTeX in order to achieve consistency between the code-base maintained by SSBee and a research article
presenting the main results. As a first case study, we implement and verify the SSP-based reduction proof for Yao's garbling scheme by Brzuska and Oechsner~\cite{X}. Moreover, since the goal of SSBee is to support the \emph{proof development} rather than merely check existing proofs, we also extend the work by Brzuska and Oechsner to compose Yao's garbling scheme with a random oblivious transfer protocol (OT) into a secure 2-party computation protocol with semi-honest security. While this result has already been proven by ...~\cite{X}, it is an interesting and meaningful extension of the original case-study by Brzuska and Oechsner since it formalizes and proves security of a secure 2-party computation protocol in SSPs for the first time. Moreover, it informs us about a natural workflow which a user might adopt.

\subsection{Workflow}
In a natural workflow, the users sketch SSP graphs and the sequences of game-hops
on paper. Here, each package has an intended behaviour that has not been fully
specified or formalized yet but that the user finds convincing. Subsequently, when
the user wants to verify their conceptual ideal, they specify the behaviour of packages
as SSBee pseudocode and construct / compose their packages into the games previously sketched on paper. In this stage, code and package compositions mature jointly, and SSBee checks the interfaces of packages to one another. Next, the user specifies all games which occur in the game hops using the previously defined packages.
SSBee immediately verifies all graph-based game-hops in this step and thus verifies that the reductions have the correct interface. Finally, the user specifies all \emph{perfect equivalence} steps, which, in most cases, are verified automatically by SSBee, using an SMT-solver beneath. In conceptually involved cases, users need to provide some \emph{core} argument from which SSBee completes the equivalence proof automatically.
%These are game-hops where one game is replaced by another whose behaviour is perfectly equivalent. While SSPs typically allow to reduce the number of perfect equivalence steps, a few still need to be carried out and they are, typically, terribly tedious and hard to carry out on paper. These steps are, indeed, conceptually similar to checking the soundness of reductions in the traditional style. While SSPs do not require carrying out such steps for each reductions, a few still remain and one needs to do them. This step is where SSBee comes close to realizing Halevi's vision. In conceptually involved cases, users need to provide some \emph{core} argument from which SSBee completes the equivalence proof automatically. Moreover, in easier cases, SSBee  generates and verifies equivalence proofs entirely automatically. Details follow.
\fi





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% Section 2
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Technical Approach and Scope}
\subsection{Induction over oracle calls}
When arguing in research articles, we typically compare the behaviour of
two cryptographic games \emph{directly}, discussing how the state in each
of the games evolves in relation to each other and why, e.g., both games
provide the same restrictions on their decryption oracle (cf. page ..in~\cite{X}).
%, e.g., by arguing that swapping two
%lines of code does not affect behaviour if the two lines operate on independent 
%state.
In order to formalize this argument, we need to show that on \emph{all} sequences of inputs, the oracles of the two games provide the same output. A clean way of formalizing this argument is via \emph{induction} over all sequences of oracle calls. We first define a relation between the state of the two games, then show that the relation holds in the beginning of the game and, additionally, that if the relation holds before an oracle call, then the relation holds also after the oracle call and the oracles of both games indeed return the same output. If the two games are very similar, the state relation can be simply equality.

Moreover, when the games are randomized, we need to argue about \emph{distributions}
of outputs rather than equality of outputs. A clean way to extend the above induction-based approach to randomized programs is by providing a bijection $f$ between
the random strings consumed by the oracle of the two games which leads to the
same output.

This \emph{invariant}-based approach with randomness mapping has been developed in the context of EasyCrypt {(\color{blue} True ?? Probably older...)} and we also adopt it in SSBee.

\subsection{Automatization}
Given a state relation and a randomness mapping, EasyCrypt additionally requires
proving that the state relation is an invariant {\color{blue} I don't like writing this.}. In turn, SSBee seeks to avoid that the user needs to do these additional proofs. Therefore, SSBee translates the pseudo-code based statements into SMTlib statements. SMTlib~\cite{X} is an interfacing language for SMT solvers and currently supported by Z3~\cite{X} and CVC5~\cite{X}. The SMT solver is tasked with showing that (i) the randomness mapping is valid (i.e., is a bijection), that (ii) the state relation is an invariant and (iii) if the state relation and the randomness mapping holds, then equal inputs lead to equal outputs.

In general, (iii) is an NP-complete problem. However, it turns out that for the instances we consider in our case studies, CVC5 are able to prove (iii) within a few seconds. An exception is the difficult proof step in Yao's garbling scheme which performs a switch from bit semantics to active/inactive semantics. Here, we need to specify an additional lemma, and CVC5 needs 30 seconds to prove (iii).

\subsection{Multi-instance packages, \textbf{for} loops and hybrid arguments}
SSBee supports multi-instance packages, \textbf{for} loops as well as hybrid arguments over polynomially many hybrids. Multi-instance games allow complex
compositions of a polynomial number of packages or number of compositions of
a priori unknown number of packages.

In the case of \textbf{for} loops, the user needs to specify additional loop invariants (EasyCrypt does this also).

For polynomially many hybrids, the user needs to specify ... as outlined in Appendix B of ... in the hybrid argument recipe.




\subsection{Limitations and future work}
no statistical game-hops (--> combine with easycrypt)
no verification of code in Coq (--> combine with SSProve somehow?)
SSP-style-only (--> inherit limitations of SSPs, no rewinding for now)




\iffalse
==== OLD OLD OLD ===



Halevi~\cite{X} joins Shoup, Bellare and Rogaway in the hope for improved quality of proofs by presenting his dream-vision of computer-assisted cryptographic proving and, implicitly, also pointing to a cause of what he calls a \emph{crisis of rigour}.
Namely, Halevi perceives designing cryptograhic schemes and reductions as \emph{creative} part of cryptographic research, while he perceives checking the soundness
of reductions as \emph{mundane}. Thus, Halevi hopes for assistance in the development
of proofs, relieving the researchers of work which they perceive as tedious: Checking the soundness of reductions shall be carried out by \emph{computers} automatically.

In a pioneering work, ... indeed enabled the checking of reductions by computers. However, unlike imagined by Halevi, checking the soundness of reductions in EasyCrypt is not \emph{automatic}. First, all games and reductions need to be formalized in the (very appealing) language of EasyCrypt. Subsequently, the equivalence between two adjacent game or the equivalence between a high-level game on the one hand and, on the other hand, a reduction composed with a game specifying an assumption, these equivalences need to be proven by first formulating an invariant, then proving in the EasyCrypt proof language that the invariant is an invariant and showing functional equivalence of two games by induction over the oracle calls\footnote{Cryptos will not understand this, needs to improve.}. EasyCrypt is a masterpiece of formal verification software and able to argue about complex programs using for-loops and also able to establish proofs of statistical closeness, both of which are notoriously difficult problems in formal verification. However, while being an extremely impressive formal verification tool, EasyCrypt is not widely used across the cryptographic community, because it does not address Halevi's vision: Using EasyCrypt, the work invested into proving equivalences / soundness of reductions is not decreased, but rather increased. EasyCrypt is expert in checking the correctness of a proof, but EasyCrypt is not a helpful tool for developing new results in cryptography.

Before accusing EasyCrpyt of not being sufficiently ambitious, we should consider criticizing Halevi for posing an overly naive vision. Proving equivalence between two (poly-time) programs is, in general, coNP-complete, and proving equivalence of two \emph{stateful}, \emph{randomized} (poly-time) programs is in the ... level of the polynomial hierarchy. Of course, SAT-solvers are apt at dealing with structured real-life instances (cf.~\cite{X,Y,Z} and references therein) of NP-complete problems, but typically do not target the ... of the polynomial hierarchy, and rarely aim at proving equivalence of randomized programs, let alone argue \emph{inductively} about interactive \emph{stateful} programs and their behaviour.

Given this dire state of affairs, a \emph{general} push-button tool for proving soundness of cryptographic reductions seems unrealistic based on the current state-of-the-art in automated solvers. However, given a state-relation and a \emph{randomness mapping}\footnote{Cryptos will not know what this is.}, proving that the state-relation is an invariant if the randomness mapping holds is ``only'' coNP-complete, and so is proving that the output of the two oracles is identical if the state-relation and the randomness mapping holds. Thus, with slightly more input from the working cryptographer, the proof might move into the range of what an automated solver is able to show, and in many cases, randomness mapping and state relation are completely trivial and can be generated automatically.

Still, computer-verification according to the above strategy requires fully writing a reduction in pseudo-code rather than describing it conceptually on a high-level, e.g., \emph{the reduction behaves like the game except for calling its \O{EVAL} oracle instead of evaluating the PRF} would need
to be replaced by a large piece of pseudo-code which consists of the full game description where the calls to the PRF are replaced by \O{EVAL} oracle calls.
Writing the reduction's pseudo-code is not infeasible given that one already
wrote the code for the game. However, we might still not be able to argue that 
the entire formalization is less tedious than carrying out the soundness proof manually.

Thus, the key question is where the above approach \emph{supports} the cryptographer
in their proof development and \emph{eases} their proof development rather than adding more work and steps to it\footnote{Again, the extra work comes with extra guarantees, and those might be worth it, but will cryptographer be willing to go this extra mile?}.

\paragraph{Our contribution.} As we show in this article, proof development is \emph{eased} via the above strategy when applied to proofs in the state-separating proofs framework (SSP~\cite{X}). [..]

\iffalse
\clearpage
OLD OLD OLD



Halevi's dream-vision of computer-aided reduction proofs in cryptography~\cite{Halevi} hopes that one day, while creativity is still reserved for humans, algorithms will be in charge of tedious steps which confirm the soundness of a reduction and only use standard techniques. An important, somewhat orthogonal, success on the path for computer-aided cryptography is that computer-aided \emph{attack-finding} has established itself firmly as a standard tool for protocol development. Most importantly, Tamarin~\cite{X} has been used to improve ...~\cite{X}, ...~\cite{X} and ...~\cite{X}, and, in particular, is used across the community also without involving an author of the original tool, see, e.g.~\cite{X}. However, computer-aided \emph{reduction proofs} have not yet joined the quest to inform protocol design quickly. In fact, to date, reduction proofs for large protocols such as TLS and MLS have not been computer-verified, let alone been helped by algorithms, and indeed, we do not yet have the tools to do so.

One core reason is that invoking the help of a computer requires formalizing one's proofs at a level of detail which is unusual for standard mathematical practice, including cryptography. For example, one might describe two security games and a construction in English, then describe a reduction in English and prove its soundness by arguing about difficult key points. In their celebrated foundational work, Bellare and Rogaway~\cite{X} encouraged the community to use \emph{code} to describe their reductions, games and game-hops and demonstrated how code-based argument allow us to obtain a neat proof for ... . This leap towards more precision enabled ... ... and ...~\cite{X} to develop the pioneer tool EasyCrypt for verifying cryptographic reductions. EasyCrypt has been used to verify ... ,... and ... as well as a proof of Yao's garbling scheme~\cite{X} as one of its largest case studies. Simultaneously, Bellare-Rogaway-style game-hopping has become an established technique in our community. Similarly, the composition frameworks Universal Composability (UC~\cite{X}) or Constructive Cryptography (CC~\cite{X}) have a large and growing user base who write their reduction proofs in these frameworks (cf. ... and references therein). This suggests that our community is willing to invest time and effort into writing clearer proofs, but the overhead currently required by EasyCrypt is still considered prohibitive. 

A recent arrival to the family of composition frameworks are \emph{state-separating proofs} (SSPs~\cite{X}) where the first large case studies have been published over the past 2 years. Concretely, SSPs have been used to provide reductions for the key derivations in both TLS~\cite{X} and MLS~\cite{X} and, most recently, to give a novel security reduction for Yao's garbling scheme~\cite{X}, demonstrating the potential feasibility to use SSPs also for secure multi-party computation.

Similarly to Bellare and Rogaway's code-based game-playing, SSPs, a refinement of Bellare-Rogaway, are formalization-friendly and have since been formalized in Coq~\cite{X} as well as in EasyCrypt~\cite{X}. In fact, SSPs have become a useful manual prototyping step for developing proofs in EasyCrypt [Dupressoir-Oechsner, personal communication :-)]. What is missing, still, however, is a formal verification tool which \emph{helps} cryptographers in the \emph{development} and \emph{execution} of their proofs and not only in the verification.

\paragraph{Our contribution.} In this paper, we provide such a tool for SSPs: SSPVerif. 
%SSPs have shown to be useful to carrying out proofs for large real-life protocols~\cite{X,Y} and thus, focusing on SSPs is a useful scope. 
We built SSPVerif, because in our development of handwritten SSP proofs for large protocols, we struggled (a) with the \emph{maintainance} of a \emph{large code-base}, (b) with verifying the \emph{consistency} of the code-base and last, but not least, (c) with carrying out code-equivalence proofs. The latter cover ... pages of the security analysis of TLS 1.3~\cite{X} and ... out of ... in the reduction proof for Yao's garbling scheme~\cite{X}. They have been completely avoided in the security reduction for MLS by the use of modular assumptions and modular top-level target security definitions~\cite{X}.

SSPVerif addresses (a) and (b) by checking consistency of packages and their compositions as well as exporting to LaTeX for a consistent research paper. For (c), code-equivalence proofs are discharged to an SMT-solver, either CVC4~\cite{X} or Z3~\cite{X}. Our case studies demonstrate that CVC4 and Z3 are, indeed, sufficiently powerful to prove typical code-equivalences with no to little manual support. 

\paragraph{Case studies}
Yao (compare with EasyCrypt), NPRF

\paragraph{Features}
for loops, multi-instance, type system

\paragraph{Limitations and future work}
SSPVerif is automatized but special-case, EasyCrypt/SSProve is general, but not automatized. Can we use SSPVerif for prototyping of invariants for EasyCrypt/SSProve? Can we use EasyCrypt to prove assumptions used in our SSP proofs. Integration with proof viewer.





\iffalse
It is no co-incidence that EasyCrypt~\cite{X}, the pioneer tool in verifying cryptographic reduction proofs, is based on Bellare and Rogaway's code-based game-hopping~\cite{X}. Namely, Bellare and Rogaway specified 



While computer-aided attack-finding has become a standard tool in protocol development


In a celebrated breakthrough, Abadi and Rogaway~\cite{X} showed that 









==== EVEN OLDER ====


Since Halevi~\cite{X} declared a \emph{crisis of rigour} for reduction proofs in cryptography
and called for tool support to leave the \emph{interesting} steps to the creative cryptographer
and the \emph{mundane} steps to a powerful automated tool, significant progress has been made
towards this vision. In particular, Dupressoir and some others~\cite{X} formalized the Bellare-Rogaway
code-based game-playing~\cite{X} approach in EasyCrypt which now allows to verify code transformations
and reduction-based game hops. 

EasyCrypt does not take care of mundane steps fully automatically, as in Halevi's vision, but rather is semi-automated
and usually requires cryptographers to write pre-conditions and post-conditions (or simply a single invariant) and prove
that if the pre-condition was satisfied before an oracle call, then the post-condition is satsified.
The reason that the SMT solver underlying EasyCrypt cannot prove code equivalence without additional
help is that conceptual high-level reasoning is not part of the solver. The present paper does not
change this state of affairs, but instead increases \emph{modularity} of the EasyCrypt technique.

Concretely, we present SSPVerif, a new verification tool for cryptographic game-hopping proofs which
is based on \emph{state-separating proofs} (SSPs), a recent, modular technique for game-hopping in pen-and-paper
proofs. SSPs structure code of cryptographic protocols and security game as stateful pieces of code
called \emph{packages} which can call one another, but otherwise do not share state. Since package
composition is associative, one can simplify complex game-hops by ``pushing code into the adversary''
(cf. Section~\ref{X}).

SSP-style techniques have already been formalized in EasyCrypt~\cite{X}, but since package are not
native in EasyCrypt, the implementation does not provide SSP-specific support and, in particular,
the SSP-style workflow. SSPs have also been formalized in SSProve~\cite{X}, a Coq-based tool specially
developed for SSPs, but no large examples have been shown in SSProve. In turn, SSPVerif has been developed
to especially support the workflow of the working SSP cryptographer.

As an example, we use Yao's garbled circuits: much larger than what is known in SSProve; Yao has already 
been done on EasyCrypt, so great test case.
\fi
\fi
\fi
%}